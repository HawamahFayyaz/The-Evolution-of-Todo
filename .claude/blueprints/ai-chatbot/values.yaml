# ============================================================
# AI CHATBOT BLUEPRINT - CONFIGURATION VALUES
# ============================================================
# Phase III Architecture:
# - Stateless backend (scales horizontally)
# - Chat history in PostgreSQL (persists across restarts)
# - MCP Server separate (independent scaling)
# - Frontend with ChatKit UI
# ============================================================

# Application Identity
app:
  name: "ai-chatbot"
  namespace: "chatbot"
  environment: "production"

# ============================================================
# FRONTEND CONFIGURATION (ChatKit UI)
# ============================================================
frontend:
  enabled: true
  name: "chatbot-frontend"

  image:
    repository: "ghcr.io/yourorg/chatbot-frontend"
    tag: "latest"
    pullPolicy: "IfNotPresent"

  replicas: 2

  port:
    container: 3000
    service: 80

  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "512Mi"

  healthCheck:
    path: "/api/health"
    initialDelaySeconds: 30
    periodSeconds: 10

  # Frontend environment (public)
  env:
    NEXT_PUBLIC_APP_NAME: "AI Assistant"
    NEXT_PUBLIC_API_URL: "https://chatbot.example.com/api"
    NEXT_PUBLIC_WS_URL: "wss://chatbot.example.com/ws"

# ============================================================
# BACKEND CONFIGURATION (FastAPI + OpenAI Agents SDK)
# ============================================================
# STATELESS: No session storage. All state in database.
# Each request includes conversation_id to load history.
backend:
  enabled: true
  name: "chatbot-backend"

  image:
    repository: "ghcr.io/yourorg/chatbot-backend"
    tag: "latest"
    pullPolicy: "IfNotPresent"

  replicas: 3  # Scale based on load

  port:
    container: 8000
    service: 8000

  resources:
    requests:
      cpu: "200m"
      memory: "512Mi"
    limits:
      cpu: "1000m"
      memory: "2Gi"  # LLM responses can be large

  healthCheck:
    path: "/health"
    initialDelaySeconds: 30
    periodSeconds: 10

  # Backend environment
  env:
    ENVIRONMENT: "production"
    LOG_LEVEL: "info"
    # MCP Server connection
    MCP_SERVER_URL: "http://chatbot-mcp-server:8001"
    # Database (from secret)
    # DATABASE_URL: from secret
    # OpenAI (from secret)
    # OPENAI_API_KEY: from secret

# ============================================================
# MCP SERVER CONFIGURATION (Model Context Protocol Tools)
# ============================================================
# Separate deployment for MCP tools
# Scales independently from main backend
mcpServer:
  enabled: true
  name: "chatbot-mcp-server"

  image:
    repository: "ghcr.io/yourorg/chatbot-mcp-server"
    tag: "latest"
    pullPolicy: "IfNotPresent"

  replicas: 2

  port:
    container: 8001
    service: 8001

  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "512Mi"

  healthCheck:
    path: "/health"
    initialDelaySeconds: 15
    periodSeconds: 10

  # MCP tools available
  tools:
    - "list_tasks"
    - "add_task"
    - "complete_task"
    - "delete_task"
    - "search_tasks"

# ============================================================
# POSTGRESQL CONFIGURATION (Chat History Persistence)
# ============================================================
# StatefulSet for stable storage
postgres:
  enabled: true
  name: "chatbot-postgres"

  image:
    repository: "postgres"
    tag: "16-alpine"

  replicas: 1  # Single instance for simplicity (use managed DB in prod)

  port:
    container: 5432
    service: 5432

  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "1Gi"

  storage:
    size: "10Gi"
    storageClassName: "standard"  # Change for cloud: gp3, pd-ssd, etc.

  # Database configuration
  database:
    name: "chatbot"
    user: "chatbot"
    # password: from secret

# ============================================================
# AUTOSCALING
# ============================================================
autoscaling:
  enabled: true

  backend:
    minReplicas: 2
    maxReplicas: 10
    targetCPU: 70
    targetMemory: 80

  mcpServer:
    minReplicas: 2
    maxReplicas: 5
    targetCPU: 70

  frontend:
    minReplicas: 2
    maxReplicas: 5
    targetCPU: 70

# ============================================================
# INGRESS
# ============================================================
ingress:
  enabled: true
  className: "nginx"
  host: "chatbot.example.com"

  tls:
    enabled: true
    secretName: "chatbot-tls"
    clusterIssuer: "letsencrypt-prod"

  # Path routing
  paths:
    frontend: "/"
    backend: "/api"
    websocket: "/ws"

# ============================================================
# SECRETS (Template - Use External Secrets in production)
# ============================================================
secrets:
  # OpenAI API Key
  OPENAI_API_KEY: "sk-your-openai-api-key-here"

  # Database
  POSTGRES_PASSWORD: "your-secure-password-here"
  DATABASE_URL: "postgresql://chatbot:your-secure-password-here@chatbot-postgres:5432/chatbot"

  # Alternative: Neon Database (serverless PostgreSQL)
  # DATABASE_URL: "postgresql://user:pass@ep-xxx.us-east-1.aws.neon.tech/chatbot?sslmode=require"

  # JWT Secret for auth
  AUTH_SECRET: "your-32-char-auth-secret-here-123"

  # Optional: Anthropic API Key (for Claude)
  # ANTHROPIC_API_KEY: "sk-ant-your-key-here"

  # Optional: Local LLM API URL
  # LOCAL_LLM_URL: "http://ollama:11434"

# ============================================================
# LABELS
# ============================================================
labels:
  common:
    "app.kubernetes.io/part-of": "ai-chatbot"
    "app.kubernetes.io/managed-by": "kubectl"
