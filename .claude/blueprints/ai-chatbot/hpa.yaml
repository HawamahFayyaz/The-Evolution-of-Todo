# ============================================================
# HORIZONTAL POD AUTOSCALER - AI Chatbot Components
# ============================================================
# Scaling strategy:
# - Backend: Scale on CPU (LLM API calls are CPU-light but I/O heavy)
# - MCP Server: Scale on CPU (tool execution)
# - Frontend: Scale on CPU (serving static files)
#
# Note: PostgreSQL should NOT be auto-scaled (use managed DB instead)
# ============================================================

---
# ==================== BACKEND HPA ====================
# Scale based on load from chat requests
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: {{app.name}}-backend-hpa
  namespace: {{app.namespace}}
  labels:
    app: {{app.name}}-backend
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{app.name}}-backend

  minReplicas: {{autoscaling.backend.minReplicas}}
  maxReplicas: {{autoscaling.backend.maxReplicas}}

  metrics:
  # CPU-based scaling
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: {{autoscaling.backend.targetCPU}}

  # Memory-based scaling (LLM responses can use memory)
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: {{autoscaling.backend.targetMemory}}

  behavior:
    # Scale up quickly for traffic spikes
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 4
        periodSeconds: 30
      selectPolicy: Max

    # Scale down slowly (LLM conversations can be long)
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 25
        periodSeconds: 60
      selectPolicy: Min

---
# ==================== MCP SERVER HPA ====================
# Scale based on tool execution load
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: {{app.name}}-mcp-server-hpa
  namespace: {{app.namespace}}
  labels:
    app: {{app.name}}-mcp-server
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{app.name}}-mcp-server

  minReplicas: {{autoscaling.mcpServer.minReplicas}}
  maxReplicas: {{autoscaling.mcpServer.maxReplicas}}

  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: {{autoscaling.mcpServer.targetCPU}}

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Pods
        value: 2
        periodSeconds: 30
    scaleDown:
      stabilizationWindowSeconds: 120
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60

---
# ==================== FRONTEND HPA ====================
# Scale based on web traffic
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: {{app.name}}-frontend-hpa
  namespace: {{app.namespace}}
  labels:
    app: {{app.name}}-frontend
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{app.name}}-frontend

  minReplicas: {{autoscaling.frontend.minReplicas}}
  maxReplicas: {{autoscaling.frontend.maxReplicas}}

  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: {{autoscaling.frontend.targetCPU}}

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 25
        periodSeconds: 60

---
# ==================== CUSTOM METRICS HPA (Advanced) ====================
# Scale based on application-specific metrics
# Requires: Prometheus Adapter installed
#
# apiVersion: autoscaling/v2
# kind: HorizontalPodAutoscaler
# metadata:
#   name: {{app.name}}-backend-custom-hpa
#   namespace: {{app.namespace}}
# spec:
#   scaleTargetRef:
#     apiVersion: apps/v1
#     kind: Deployment
#     name: {{app.name}}-backend
#   minReplicas: 2
#   maxReplicas: 20
#   metrics:
#   # Scale based on active conversations
#   - type: Pods
#     pods:
#       metric:
#         name: active_conversations
#       target:
#         type: AverageValue
#         averageValue: "10"  # 10 concurrent conversations per pod
#
#   # Scale based on pending chat requests
#   - type: External
#     external:
#       metric:
#         name: http_requests_pending
#         selector:
#           matchLabels:
#             service: "chatbot-backend"
#       target:
#         type: Value
#         value: "100"  # Scale when > 100 pending requests
